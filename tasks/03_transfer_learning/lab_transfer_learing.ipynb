{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 3 \n",
    "\n",
    "# Классификация с использованием BERT\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Щербаков Василий Сергеевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете классифицировать пары вопросов из stack overflow на предмет дубликатов.\n",
    "Чтобы получить гораздо более высокое качество на гораздо меньшем количестве данных, чем DSSM, предлагается дообучать предобученную модель BERT.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Transformers](https://github.com/huggingface/transformers).\n",
    " \n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве task3_data.zip, который состоит из:\n",
    "\n",
    "* train.tsv - обучающая выборка. В каждой строке записаны: <вопрос 1>, <вопрос 2>, <таргет>\n",
    "\n",
    "* validation.tsv - dev выборка, которую можно использовать для подбора гиперпарамеров; например, для ранней остановки. В каждой строке через табуляцию записаны: , <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...\n",
    "\n",
    "* test.tsv - тестовая выборка, по которой оценивается итоговое качество. В каждой строке через табуляцию записаны: , <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...\n",
    "\n",
    "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/1Owb5Vpv7mVjksYo7gD9VuHkMETkzhIdr/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с теми же данными, которые были в первом задании. А также будем учиться классифицировать пары вопросов аналогично третьей части в первом задании. Теперь выборка для обучения сгенерирована заранее :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь к папке с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание данных для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table(os.path.join(DATA_PATH, 'train.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель **BERT** использует специальный токенизатор Wordpiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: **BertTokenizer** и **BertTokenizerFast**. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
    "\n",
    "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора. \n",
    "\n",
    "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
    "\n",
    "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации пар предложений необходимо привести примеры к виду: \n",
    "\n",
    "**[CLS] sent 1 [SEP] sent2 [SEP]**, \n",
    "\n",
    "где последний [SEP] можно опустить - в некоторых реализациях его используют, в некоторых нет. Существенного влияния на качество он не оказывает.\n",
    "\n",
    "Предлагается привести все предложения из обучения к данному виду перед созданием Dataset. Для этого удобно использовать метод **tokenizer.encode_plus**, который сам вставляет специальные специальные токены [CLS], [SEP] в числовое представление примера. \n",
    "\n",
    "Кроме того, данный метод сразу формирует для наших примеров сегментные эмбеддинги - т.е. сопоставляет всем токенам первого предложения эмбеддинг **А**, и всем токенам второго предложения эмбеддинг **Б**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(query1, query2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query1: query text\n",
    "        query2: second query text\n",
    "        \n",
    "    Returns:\n",
    "        obj: dict {'input_ids': [0, 1, 2, 2, 1], 'token_type_ids': [0, 0, 1, 1, 1]}\n",
    "    \"\"\"\n",
    "    return tokenizer.encode_plus(query1, query2)\n",
    "\n",
    "tests.test_encode(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\v.sherbakov\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\std.py:706: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600000/600000 [03:00<00:00, 3325.87it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "\n",
    "train['enc'] = train.progress_apply(lambda x: encode(x['question_1'], x['question_2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируйте количество токенов в получившихся представлениях объектов, выберите максимальный порог длины, затем обрежьте все представления по этому порогу. Это необходимо для более разумного использования видеопамяти.\n",
    "\n",
    "**hint:** можно использовать квантиль из **np.percentile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>target</th>\n",
       "      <th>enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Logstash split xml into array</td>\n",
       "      <td>Excel Interop: Range.FormatConditions.Add thro...</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Resumable upload to google drive from JS using...</td>\n",
       "      <td>Google Drive upload with CORS</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Database design for a survey</td>\n",
       "      <td>How to display data from two tables in a SQL S...</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Socket IO net::ERR_CONNECTION_REFUSED</td>\n",
       "      <td>How can I put a control in the JTableHeader of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>How to create an HTML checkbox with a clickabl...</td>\n",
       "      <td>How to open/create UIManagedDocument synchrono...</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>animate the fill color in cylinder from bottom...</td>\n",
       "      <td>Fill color with animation</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>Using %f to print an integer variable</td>\n",
       "      <td>using printf to print out floating values</td>\n",
       "      <td>1</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>How to handle session end in global.asax?</td>\n",
       "      <td>LOAD SQL Table from flat file</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>Is there a good reason for always enclosing a ...</td>\n",
       "      <td>Access CKEditor iframe's style tags with jQuery</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>Cannot set the value of read-only property 'jn...</td>\n",
       "      <td>Google Chrome wrong character - bug?</td>\n",
       "      <td>0</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599959 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question_1  \\\n",
       "41                          Logstash split xml into array   \n",
       "42      Resumable upload to google drive from JS using...   \n",
       "43                           Database design for a survey   \n",
       "44                  Socket IO net::ERR_CONNECTION_REFUSED   \n",
       "45      How to create an HTML checkbox with a clickabl...   \n",
       "...                                                   ...   \n",
       "599995  animate the fill color in cylinder from bottom...   \n",
       "599996              Using %f to print an integer variable   \n",
       "599997          How to handle session end in global.asax?   \n",
       "599998  Is there a good reason for always enclosing a ...   \n",
       "599999  Cannot set the value of read-only property 'jn...   \n",
       "\n",
       "                                               question_2  target  \\\n",
       "41      Excel Interop: Range.FormatConditions.Add thro...       0   \n",
       "42                          Google Drive upload with CORS       1   \n",
       "43      How to display data from two tables in a SQL S...       0   \n",
       "44      How can I put a control in the JTableHeader of...       0   \n",
       "45      How to open/create UIManagedDocument synchrono...       0   \n",
       "...                                                   ...     ...   \n",
       "599995                          Fill color with animation       1   \n",
       "599996          using printf to print out floating values       1   \n",
       "599997                      LOAD SQL Table from flat file       0   \n",
       "599998    Access CKEditor iframe's style tags with jQuery       0   \n",
       "599999               Google Chrome wrong character - bug?       0   \n",
       "\n",
       "                                                enc  \n",
       "41      [input_ids, token_type_ids, attention_mask]  \n",
       "42      [input_ids, token_type_ids, attention_mask]  \n",
       "43      [input_ids, token_type_ids, attention_mask]  \n",
       "44      [input_ids, token_type_ids, attention_mask]  \n",
       "45      [input_ids, token_type_ids, attention_mask]  \n",
       "...                                             ...  \n",
       "599995  [input_ids, token_type_ids, attention_mask]  \n",
       "599996  [input_ids, token_type_ids, attention_mask]  \n",
       "599997  [input_ids, token_type_ids, attention_mask]  \n",
       "599998  [input_ids, token_type_ids, attention_mask]  \n",
       "599999  [input_ids, token_type_ids, attention_mask]  \n",
       "\n",
       "[599959 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens_list = [len(token['input_ids']) for token in train['enc']]\n",
    "MAXLEN = np.percentile(lens_list,95)\n",
    "print(MAXLEN)\n",
    "train.truncate(MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Задание пайплайна обучения (2 балла)\n",
    "\n",
    "**Внимание**. За эту часть можно получить ненулевой балл, только при демонстрации того, что ваша модель хоть как-то обучается и  работает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет и загрузчик\n",
    "\n",
    "Создайте датасет, из которого **DataLoader** будет брать объекты для формирования батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus, targets):\n",
    "        self.__encoded = corpus['enc']\n",
    "        self.__targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            obj: (input_ids, token_type_ids, target)\n",
    "        \"\"\"\n",
    "        input_ids = torch.Tensor(self.__encoded[idx]['input_ids'])\n",
    "        token_type_ids = torch.Tensor(self.__encoded[idx]['token_type_ids'])\n",
    "        target = self.__targets[idx]\n",
    "        return (input_ids, token_type_ids, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyDataset(train, train['target'])\n",
    "\n",
    "tests.test_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте технику динамического паддинга батчей, используя функцию **collate_fn**, которую можно передать как одноименный параметр в класс **DataLoader**.\n",
    "\n",
    "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch, pad_idx=0):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            batch: list of objects\n",
    "            pad_idx: padding idx\n",
    "        Returns:\n",
    "            padded ids, token_type_ids, labels\n",
    "    \"\"\"\n",
    "    input_ids, token_types, labels = zip(*batch)\n",
    "    input_ids = [torch.Tensor(input_id) for input_id in input_ids]\n",
    "    token_types = [torch.Tensor(token_type) for token_type in token_types]\n",
    "    labels = torch.Tensor(labels)\n",
    "    padded_input_ids = pad_sequence(input_ids, padding_value = pad_idx, batch_first = True) \n",
    "    padded_token_types = pad_sequence(token_types, batch_first = True) \n",
    "    res = (padded_input_ids, padded_token_types, labels)\n",
    "    return res\n",
    "\n",
    "tests.test_collator(ds, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "dataloader = DataLoader(ds, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "\n",
    "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Но гораздо более гибкий подход --- использовать энкодер BERT и, по необходимости, входной слой BERT.\n",
    "\n",
    "Существует два способа задания модели:\n",
    "* с помощью конфига **transformers.BertConfig**, в котором указываются все гиперпараметры модели\n",
    "* с помощью подгрузки предобученной модели. Можно загружать как свои предобученные модели, указав путь, так и готовые предобученные модели, указав название конфигурации. В данном задании мы уже выбрали как модель базовую конфигурацию *BERT base*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e9b080a54e4008840f7dec1fac405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите модель-обертку, которая:\n",
    "* принимает на вход название конфигурации (или путь к предобученной модели) и загружает как свой внутренний слой, обычно называемый *backbone* слоем\n",
    "* создает голову для классификации\n",
    "* при вызове метода **forward** использует векторное представление токена [CLS] с последнего слоя для классификации\n",
    "\n",
    "На вход BERT принимает:\n",
    "* input_ids --- непосредственно индексы ваших токенов в словаре\n",
    "* attention_mask --- булеву маску со значениями FALSE для всех PAD_IDX токенов\n",
    "* token_type_ids --- индексы принадлежности токена к 1 или 2 вопросу\n",
    "\n",
    "**hint:** в статье про BERT авторы опустили следующий архитектурный момент - представление CLS токена используется для NSP задачи, но перед классификацией оно проходит через так называемый **pooler** слой - линейный слой с *tanh* в качестве функции активации, который сохраняет размерность (т.е. на выходе оставляет hidden size значений). Если вы хотите использовать выход именно *pooler* слоя, нужно использовать вектор, получаемый из энкодера как второй элемент кортежа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert, n_classes=1):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        pass\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path, n_classes=1):\n",
    "        bert = BertModel.from_pretrained(path)\n",
    "        return cls(bert, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                input_ids: token ids, shape = [batch_size, sequence_length]\n",
    "                attention_mask: masks out padding tokens, shape = [batch_size, sequence_length]\n",
    "                token_type_ids: segmend ids, shape = [batch_size, sequence_length]\n",
    "            Returns:\n",
    "                predictions, shape [batch_size]\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = BERTClassifier.from_pretrained(BERT_MODEL, n_classes=1).to(device)\n",
    "\n",
    "tests.test_model(dataloader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизатор\n",
    "\n",
    "Для оптимизации **BERT** будем использовать **AdamW** c увеличенным learning rate'ом для параметров головы-классификатора. \n",
    "\n",
    "Отличие **AdamW** от **Adam** заключается в более корректной реализации $l_2$ регуляризации, которая задается параметром **weight_decay** при инициализации.\n",
    "\n",
    "Параметры необходимо объединить на три группы:\n",
    "\n",
    "* параметры, которым нужен weight decay --- все параметры из backbone, кроме сдвигов (bias) и LayerNorm слоев.\n",
    "* остальные парамеры из backbone\n",
    "* параметры головы классификации, для которых мы будем задавать гораздо больший learning rate\n",
    "\n",
    "Будем использовать **model.named_parameters()**, чтобы разделить параметры на три группы, исходя из названий слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_DECAY = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "def is_backbone(name):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "def needs_decay(name):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "def get_optimizer(model, lr, weight_decay, head_lr):\n",
    "    grouped_parameters = [\n",
    "        {\n",
    "            'params': [param for name, param in model.named_parameters() if is_backbone(name) and needs_decay(name)],\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "        },\n",
    "        {\n",
    "            'params': [param for name, param in model.named_parameters() if is_backbone(name) and not needs_decay(name)],\n",
    "            'lr': lr,\n",
    "            'weight_decay': 0.,\n",
    "        },\n",
    "        {\n",
    "            'params': [param for name, param in model.named_parameters() if not is_backbone(name)],\n",
    "            'lr': head_lr,\n",
    "            'weight_decay': weight_decay,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(grouped_parameters, lr, weight_decay=weight_decay)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "LEARNING_RATE = \n",
    "WEIGHT_DECAY = \n",
    "HEAD_LEARNING_RATE = \n",
    "\n",
    "optimizer = get_optimizer(model, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, head_lr=HEAD_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler\n",
    "\n",
    "\n",
    "Также необходимо задать расписание для learning rate. Для **BERT** используется **linear warmup**. \n",
    "\n",
    "В **transformers** есть реализация **linear warmup** с помощью метода **transformers.get_linear_schedule_with_warmup**, в которой learning rate стартует с 0, и в течение **num_warmup_steps** линейно возрастает до значения, указанного в качестве стартового в оптимизаторе. Затем в течение **num_training_steps - num_warmup_steps** learning rate линейно падает до 0.\n",
    "\n",
    "Используйте *dataloader.dataset* и *dataloader.batch_size*, чтобы рассчитать *num_training_steps* исходя из количества эпох. В случае нашей задачи одной эпохи должно быть достаточно для обучения модели.\n",
    "\n",
    "В случае ограниченного количества видеопамяти может возникнуть ситуация, при которой батч нужного размера не влезает в видеокарту. Для таких ситуаций предлагается использовать аккумуляцию градиента - накапливание градиента в течение *accumulation_steps* с последующим шагом спуска. Т.е. делать *(loss / accumulation_steps).backward()* для каждого батча, и при этом каждые *accumulation_steps* шагов делать *optimizer.step()*.\n",
    "\n",
    "При обучении количество шагов warmup выбирают либо как 10000 шагов, либо как 0.01% или 0.06% от всех шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "def get_scheduler(optimizer, dataloader, n_epochs, accumulation_steps, warmup_percentage):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    pass\n",
    "\n",
    "N_EPOCHS = 1\n",
    "ACCUMULATION_STEPS = \n",
    "WARMUP_PERCENTAGE = \n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    optimizer, dataloader, n_epochs=N_EPOCHS, accumulation_steps=ACCUMULATION_STEPS, warmup_percentage=WARMUP_PERCENTAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки качества модели необходимо использовать подготовленный для задания **Evaluator**. Важный момент: при использовании, evaluator переводит модель в режим валидации: model.eval(). Во время обучения необходимо самостоятельно переключать ее на model.train() после каждого использования.\n",
    "\n",
    "На вход evaluator принимает вашу модель и device (CUDA или CPU), на котором необходимо считать результаты моделирования. \n",
    "\n",
    "При использовании evaluator можно использовать BATCH_SIZE значительно большего размера, потому что отпадает необходимость считать градиенты для параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Evaluator\n",
    "\n",
    "evaluator = Evaluator(os.path.join(DATA_PATH, 'validation.tsv'), tokenizer, maxlen=MAXLEN, batch_size=1024)\n",
    "metrics = evaluator(model, device, verbose=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный **evaluator** предлагается использовать не только для оценки итогового качества, но также для вывода промежуточных результатов на dev сете в логи с помощью **torch.utils.tensorboard.SummaryWriter**.\n",
    "\n",
    "Перед обучением необходимо создать объект данного класса, указав папку для записи логов.\n",
    "\n",
    "Во время обучения через каждые $10000$ объектов необходимо записывать значения метрик в логи с помощью методов **writer.add_scalars**. Кроме того, необходимо записывать значение функционала ошибки на каждом батче во время обучения с помощью метода **writer.add_scalar**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Обучение модели (7 баллов)\n",
    "\n",
    "Ниже предлагаются примерные значения гиперпараметров, приводящие к необходимым метрикам качества. Для подбора точных значений гиперпараметров предлагается использовать *dev set*.\n",
    "\n",
    "**Гиперпараметры для обучения:**\n",
    "\n",
    "* размер батча в $\\{32, 64\\}$\n",
    "* клиппинг нормы градиента (используйте **torch.nn.utils.clip_grad_norm_**)\n",
    "* шаг обучения в $\\{$1e-5, 2e-5, 3e-5, 4e-5$\\}$\n",
    "* weight decay в $\\{$1e-2, 1e-3, 1e-4$\\}$\n",
    "* warmup percentage в $\\{0.01, 0.06\\}$\n",
    "* шаг обучения для головы-классификатора в $\\{10, 50, 100\\}$ раз больше, чем для остальных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_metrics = evaluator(model, device)\n",
    "dev_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = Evaluator(os.path.join(DATA_PATH, 'test.tsv'), tokenizer, maxlen=MAXLEN, batch_size=1024)\n",
    "test_metrics = test_evaluator(model, device, verbose=True)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет засчитано на полный балл при *Hits@1* на *test set* больше $0.6$. Необходимо приложить логи из тензорборда, а также скриншот этих самых логов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть (до 6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMO-подобная архитектура для головы-классификатора (до 2 баллов)\n",
    "\n",
    "Реализуйте и обучите ELMO-подобную архитектуру для головы-классификатора: берутся все 13 векторных представлений CLS токена, и затем с обучаемыми софтмакс-нормализуемыми весами складываются перед линейным классификатором. Дообучаются ВСЕ веса, включая сам берт. Можно попробовать зафризить исходный берт, но с большой вероятностью наибольшее качество достигается при дообучении всего.\n",
    "\n",
    "Рекомендуется инициализировать обучаемые веса равными значениями, а также наряду с головой-классификатором присвоить им learning rate, значительно больший по значению, чем у энкодера.\n",
    "\n",
    "Требуется получить качество хотя бы примерно такое же (а желательно и выше), чем при основной архитектуре. Может понадобиться больше эпох для обучения!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение качества на том же наборе данных (до 2 баллов)\n",
    "\n",
    "Можно использовать любые способа для улучшения качества, КРОМЕ изменения датасетов. Например:\n",
    "\n",
    "* Multi-sample Dropout --- при обучении, перед головой классификации итоговый вектор прогоняется через Dropout *n*-ное количество раз, каждый из полученных векторов проводится через голову классификации, и результаты усредняются.\n",
    "* Изменения в архитектуре энкодера --- попробовать large конфигурацию, поменять функцию активации и прочие гиперпараметры, взять предобученный альберт из huggingface c пошаренными весами в энкодере\n",
    "* попробовать дотюнить bert на MLM задачу (как в ULMFiT) перед дообучением на задачу классификации\n",
    "* попробовать другие головы классификации - elmo-like голову, макс/авг пулинг по всем токенам или по всем векторам CLS токена, конкатенацию векторов CLS токена; сверточную сеть для классификации\n",
    "\n",
    "Требуется получить Hits@1 $ \\geqslant 0.65$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение качества с генерацией нового тренировочного набора (до 2 баллов)\n",
    "\n",
    "Для формирования тренировочной выборки в данном задании использовался *train_data* из первой домашней работы. Были взяты $100000$ пар дубликатов, и для первого дубликата из каждой пары также было сгенерировано 5 отрицательных примеров с помощью негативного сэмплирования. Предлагается самостоятельно сгенерировать тренировочную выборку, подобрать наилучший размер, а также количество негативных сэмплов.\n",
    "\n",
    "Валидироваться надо на тех же самых датасетах.\n",
    "\n",
    "Требуется получить Hits@1 $\\geqslant0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.: возможна корректировка дополнительных баллов по результатам выполнения бонусов. Рекомендуется в любом случае попробовать их сделать, даже если не получится получить нужное значение метрики :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
